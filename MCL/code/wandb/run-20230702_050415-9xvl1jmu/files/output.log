
Epoch 1/1000
Traceback (most recent call last):
  File "main.py", line 102, in <module>
    output_information = Procedure.Metric_train_original(args, dataset, model, metric, epoch, sampler, w)
  File "/data1/deepdog/project/GFML/MCL/code/Procedure.py", line 32, in Metric_train_original
    metric_loss, reg_loss = metric.stageOne(S, num_items_per_user)
  File "/data1/deepdog/project/GFML/MCL/code/utils.py", line 32, in stageOne
    self.opt.step()
  File "/data1/deepdog/anaconda3/envs/mcl/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/data1/deepdog/anaconda3/envs/mcl/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/data1/deepdog/anaconda3/envs/mcl/lib/python3.7/site-packages/torch/optim/adam.py", line 118, in step
    eps=group['eps'])
  File "/data1/deepdog/anaconda3/envs/mcl/lib/python3.7/site-packages/torch/optim/_functional.py", line 86, in adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt